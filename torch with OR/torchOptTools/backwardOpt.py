#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2025/9/2 17:15
# @Author  : xueyudian
# @File    : backwardOpt.py
# @Project : OR

import torch
import numpy as np
from typing import Dict, Callable, Optional, Union


class GradientModifier:
    """
    æ¢¯åº¦ä¿®æ”¹å·¥å…·ç±»ï¼Œæ”¯æŒå¤šç§æ¢¯åº¦è°ƒæ•´ç­–ç•¥ä»¥ä¼˜åŒ–è®­ç»ƒæ•ˆæœ
    æ”¯æŒï¼šæ¢¯åº¦è£å‰ªã€é˜ˆå€¼è¿‡æ»¤ã€ç¼©æ”¾ã€ç¨€ç–åŒ–ã€å¹³æ»‘ã€ä¸å¯å¯¼å‡½æ•°æ¢¯åº¦æ›¿æ¢
    å·²ä¿®å¤ï¼šéå¶å­å¼ é‡æ¢¯åº¦ä¿ç•™è­¦å‘Šã€æ¢¯åº¦è°ƒç”¨å‚æ•°ä¸åŒ¹é…é—®é¢˜
    """

    def __init__(self):
        # ä¸å¯å¯¼å‡½æ•°æ¢¯åº¦æ›¿æ¢è§„åˆ™ï¼š{å‡½æ•°æ ‡è¯†: æ¢¯åº¦æ›¿æ¢å‡½æ•°}
        self.undef_func_grad_rules: Dict[str, Callable] = {}
        # éœ€è¦ç‰¹æ®Šå¤„ç†çš„å¼ é‡ï¼ˆè‡ªåŠ¨ä¸ºéå¶å­å¼ é‡å¯ç”¨retain_gradï¼‰
        self.special_tensors = set()

    def register_undef_func_rule(self, func_name: str, grad_func: Callable):
        """æ³¨å†Œä¸å¯å¯¼å‡½æ•°çš„æ¢¯åº¦æ›¿æ¢è§„åˆ™"""
        self.undef_func_grad_rules[func_name] = grad_func
        print(f"âœ… å·²æ³¨å†Œä¸å¯å¯¼å‡½æ•°è§„åˆ™ï¼šfunc_name={func_name}")

    def register_special_tensor(self, tensor: torch.Tensor, tensor_name: str = "unknown"):
        """æ³¨å†Œç‰¹æ®Šå¼ é‡ï¼ˆè‡ªåŠ¨ä¸ºéå¶å­å¼ é‡å¯ç”¨retain_gradï¼‰"""
        if not tensor.is_leaf:
            tensor.retain_grad()
            print(f"â„¹ï¸ å¼ é‡[{tensor_name}]ä¸ºéå¶å­å¼ é‡ï¼Œå·²è‡ªåŠ¨å¯ç”¨retain_grad")
        self.special_tensors.add(tensor)
        print(f"âœ… å·²æ³¨å†Œç‰¹æ®Šå¼ é‡ï¼š{tensor_name}ï¼Œå½¢çŠ¶={tensor.shape}ï¼Œæ˜¯å¦å¶å­å¼ é‡={tensor.is_leaf}")

    def clip_by_norm(self, parameters, max_norm: float = 1.0, norm_type: float = 2.0):
        """æŒ‰èŒƒæ•°è£å‰ªæ¢¯åº¦"""
        before_norm = torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type, error_if_nonfinite=False)
        print(f"ğŸ“ æ¢¯åº¦èŒƒæ•°è£å‰ªï¼šè£å‰ªå‰æ€»èŒƒæ•°={before_norm:.4f}ï¼Œæœ€å¤§å…è®¸èŒƒæ•°={max_norm}")

    def clip_by_value(self, parameters, min_val: float = -1.0, max_val: float = 1.0):
        """æŒ‰å€¼è£å‰ªæ¢¯åº¦"""
        grad_stats = []
        for p in parameters:
            if p.grad is not None:
                before_min = p.grad.data.min().item()
                before_max = p.grad.data.max().item()
                p.grad.data.clamp_(min_val, max_val)
                after_min = p.grad.data.min().item()
                after_max = p.grad.data.max().item()
                grad_stats.append({
                    "param": p.__class__.__name__,
                    "before_min": before_min,
                    "before_max": before_max,
                    "after_min": after_min,
                    "after_max": after_max
                })
        print(f"ğŸ“Š æ¢¯åº¦å€¼è£å‰ªï¼šèŒƒå›´=[{min_val}, {max_val}]")
        for stat in grad_stats:
            print(
                f"  - {stat['param']}ï¼šè£å‰ªå‰[{stat['before_min']:.4f}, {stat['before_max']:.4f}] â†’ è£å‰ªå[{stat['after_min']:.4f}, {stat['after_max']:.4f}]")

    def filter_small_grads(self, parameters, threshold: float = 1e-4):
        """è¿‡æ»¤è¿‡å°æ¢¯åº¦ï¼ˆç½®0ï¼‰"""
        zero_ratio_stats = []
        for p in parameters:
            if p.grad is not None:
                total = p.grad.data.numel()
                zero_before = (p.grad.data == 0).sum().item()
                mask = torch.abs(p.grad.data) < threshold
                p.grad.data.masked_fill_(mask, 0.0)
                zero_after = (p.grad.data == 0).sum().item()
                zero_ratio = (zero_after / total) * 100
                zero_ratio_stats.append({
                    "param": p.__class__.__name__,
                    "zero_ratio(%)": zero_ratio,
                    "zero_count": zero_after - zero_before
                })
        print(f"ğŸ” å°æ¢¯åº¦è¿‡æ»¤ï¼šé˜ˆå€¼={threshold}")
        for stat in zero_ratio_stats:
            print(f"  - {stat['param']}ï¼šæ–°å¢é›¶æ¢¯åº¦æ•°={stat['zero_count']}ï¼Œæ€»é›¶æ¢¯åº¦å æ¯”={stat['zero_ratio(%)']:.2f}%")

    def scale_grads(self, parameters, scale_factor: float = 1.0):
        """ç¼©æ”¾æ¢¯åº¦"""
        grad_mean_stats = []
        for p in parameters:
            if p.grad is not None:
                before_mean = p.grad.data.mean().item()
                p.grad.data.mul_(scale_factor)
                after_mean = p.grad.data.mean().item()
                grad_mean_stats.append({
                    "param": p.__class__.__name__,
                    "before_mean": before_mean,
                    "after_mean": after_mean
                })
        print(f"ğŸ“ˆ æ¢¯åº¦ç¼©æ”¾ï¼šç¼©æ”¾å› å­={scale_factor}")
        for stat in grad_mean_stats:
            print(f"  - {stat['param']}ï¼šæ¢¯åº¦å‡å€¼ {stat['before_mean']:.6f} â†’ {stat['after_mean']:.6f}")

    def replace_undef_func_grads(self):
        """æ›¿æ¢ä¸å¯å¯¼å‡½æ•°çš„è¾“å…¥å¼ é‡æ¢¯åº¦"""
        print(f"ğŸ”„ å¼€å§‹æ›¿æ¢ä¸å¯å¯¼å‡½æ•°æ¢¯åº¦ï¼Œå…±{len(self.special_tensors)}ä¸ªç‰¹æ®Šå¼ é‡å¾…å¤„ç†")
        for idx, tensor in enumerate(self.special_tensors, 1):
            if tensor.grad is None:
                print(f"  âŒ å¼ é‡{idx}ï¼šæ¢¯åº¦ä¸ºNoneï¼Œè·³è¿‡æ›¿æ¢")
                continue

            # åŒ¹é…ä¸å¯å¯¼å‡½æ•°
            matched = False
            for func_name, grad_func in self.undef_func_grad_rules.items():
                grad_fn_name = tensor._grad_fn.__class__.__name__.lower()
                if func_name in grad_fn_name:
                    before_grad_mean = tensor.grad.data.mean().item()
                    tensor.grad.data = grad_func(tensor.grad.data)
                    after_grad_mean = tensor.grad.data.mean().item()
                    print(f"  âœ… å¼ é‡{idx}ï¼šåŒ¹é…å‡½æ•°[{func_name}]ï¼ˆæ¢¯åº¦å‡½æ•°ï¼š{tensor._grad_fn.__class__.__name__}ï¼‰")
                    print(f"     æ¢¯åº¦å‡å€¼ {before_grad_mean:.6f} â†’ {after_grad_mean:.6f}")
                    matched = True
                    break
            if not matched:
                print(f"  âš ï¸  å¼ é‡{idx}ï¼šæœªåŒ¹é…åˆ°ä»»ä½•æ³¨å†Œçš„ä¸å¯å¯¼å‡½æ•°è§„åˆ™")

    def apply_sparsity(self, parameters, sparsity_ratio: float = 0.1):
        """æ¢¯åº¦ç¨€ç–åŒ–ï¼ˆéšæœºç½®0ï¼‰"""
        sparsity_stats = []
        for p in parameters:
            if p.grad is not None:
                total = p.grad.data.numel()
                mask = torch.rand_like(p.grad.data) < sparsity_ratio
                zero_count = mask.sum().item()
                p.grad.data.masked_fill_(mask, 0.0)
                sparsity = (zero_count / total) * 100
                sparsity_stats.append({
                    "param": p.__class__.__name__,
                    "sparsity(%)": sparsity,
                    "zero_count": zero_count
                })
        print(f"ğŸŒ«ï¸  æ¢¯åº¦ç¨€ç–åŒ–ï¼šç›®æ ‡ç¨€ç–æ¯”ä¾‹={sparsity_ratio * 100}%")
        for stat in sparsity_stats:
            print(f"  - {stat['param']}ï¼šå®é™…ç¨€ç–å æ¯”={stat['sparsity(%)']:.2f}%ï¼Œç½®é›¶æ•°é‡={stat['zero_count']}")

    def smooth_grads(self, parameters, alpha: float = 0.9):
        """æ¢¯åº¦å¹³æ»‘ï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰"""
        if not hasattr(self, 'prev_grads'):
            self.prev_grads = {}
            print(f"ğŸ“ åˆå§‹åŒ–æ¢¯åº¦å†å²ç¼“å­˜ï¼Œå¹³æ»‘ç³»æ•°alpha={alpha}")

        smooth_stats = []
        for p in parameters:
            if p.grad is None:
                continue

            param_id = id(p)
            before_mean = p.grad.data.mean().item()
            # åˆå§‹åŒ–å†å²æ¢¯åº¦ï¼ˆé¦–æ¬¡å¤„ç†ï¼‰
            if param_id not in self.prev_grads:
                self.prev_grads[param_id] = torch.zeros_like(p.grad.data)
                print(f"  ğŸ†• é¦–æ¬¡å¤„ç†{p.__class__.__name__}ï¼Œåˆå§‹åŒ–å†å²æ¢¯åº¦")

            # è®¡ç®—å¹³æ»‘åæ¢¯åº¦
            smoothed_grad = alpha * self.prev_grads[param_id] + (1 - alpha) * p.grad.data
            after_mean = smoothed_grad.mean().item()
            # æ›´æ–°å½“å‰æ¢¯åº¦å’Œå†å²ç¼“å­˜
            p.grad.data = smoothed_grad
            self.prev_grads[param_id] = smoothed_grad.clone()

            smooth_stats.append({
                "param": p.__class__.__name__,
                "before_mean": before_mean,
                "after_mean": after_mean
            })

        print(f"ğŸ§½ æ¢¯åº¦å¹³æ»‘ï¼šalpha={alpha}")
        for stat in smooth_stats:
            print(f"  - {stat['param']}ï¼šæ¢¯åº¦å‡å€¼ {stat['before_mean']:.6f} â†’ {stat['after_mean']:.6f}")

    def apply(self, parameters, strategies: Optional[Dict[str, Union[Dict, bool]]] = None):
        """ä¸€é”®åº”ç”¨å¤šç§æ¢¯åº¦ä¿®æ”¹ç­–ç•¥"""
        # é»˜è®¤ç­–ç•¥é…ç½®
        default_strategies = {
            "clip_by_norm": {"max_norm": 1.0, "norm_type": 2.0},
            "filter_small_grads": {"threshold": 1e-4},
            "replace_undef_func_grads": True
        }
        # åˆå¹¶ç­–ç•¥ï¼ˆç”¨æˆ·ç­–ç•¥è¦†ç›–é»˜è®¤ï¼‰
        final_strategies = {**default_strategies, **(strategies or {})}

        print("=" * 60)
        print("ğŸš€ å¼€å§‹æ‰§è¡Œæ¢¯åº¦ä¿®æ”¹ç­–ç•¥ï¼Œå…±å¯ç”¨{}ç§ç­–ç•¥".format(sum(1 for v in final_strategies.values() if v)))
        print("=" * 60)

        for strategy, config in final_strategies.items():
            if not config:
                print(f"\nâŒ è·³è¿‡ç­–ç•¥ï¼š{strategy}ï¼ˆå·²ç¦ç”¨ï¼‰")
                continue

            # å¤„ç†é…ç½®æ ¼å¼
            if isinstance(config, bool):
                config = {}
            print(f"\nğŸ“Œ æ­£åœ¨æ‰§è¡Œç­–ç•¥ï¼š{strategy}ï¼Œé…ç½®={config}")

            # è°ƒç”¨å¯¹åº”ç­–ç•¥æ–¹æ³•
            if hasattr(self, strategy):
                try:
                    if strategy == "replace_undef_func_grads":
                        getattr(self, strategy)()  # æ— parameterså‚æ•°
                    else:
                        getattr(self, strategy)(parameters, **config)
                    print(f"âœ… ç­–ç•¥{strategy}æ‰§è¡Œå®Œæˆ")
                except Exception as e:
                    print(f"âŒ ç­–ç•¥{strategy}æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
            else:
                print(f"âŒ æœªçŸ¥ç­–ç•¥ï¼š{strategy}ï¼Œè·³è¿‡æ‰§è¡Œ")

        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æ¢¯åº¦ä¿®æ”¹ç­–ç•¥æ‰§è¡Œå®Œæ¯•")
        print("=" * 60)


# ------------------------------
# è¯¦ç»†æµ‹è¯•æ¡ˆä¾‹ï¼ˆæ‰“å°å®Œæ•´è¿‡ç¨‹ç»“æœï¼‰
# ------------------------------
if __name__ == "__main__":
    # 1. åˆå§‹åŒ–åŸºç¡€ç»„ä»¶
    torch.manual_seed(42)  # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
    print("=" * 80)
    print("ğŸ“‹ åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒï¼š")
    print(f"  - PyTorchç‰ˆæœ¬ï¼š{torch.__version__}")
    print(f"  - éšæœºç§å­ï¼š42")
    print("=" * 80)

    # 2. åˆ›å»ºæ¨¡å‹ã€æ¢¯åº¦ä¿®æ”¹å™¨ã€ä¼˜åŒ–å™¨
    model = torch.nn.Sequential(
        torch.nn.Linear(10, 20),  # ç¬¬ä¸€å±‚ï¼šè¾“å…¥10ç»´â†’è¾“å‡º20ç»´
        torch.nn.ReLU(),
        torch.nn.Linear(20, 2)  # ç¬¬äºŒå±‚ï¼šè¾“å…¥20ç»´â†’è¾“å‡º2ç»´ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
    )
    grad_modifier = GradientModifier()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    print(f"\nğŸ“¦ æ¨¡å‹ç»“æ„ï¼š")
    for idx, layer in enumerate(model, 1):
        print(
            f"  {idx}. {layer.__class__.__name__} â†’ è¾“å…¥ç»´åº¦ï¼š{layer.in_features if hasattr(layer, 'in_features') else 'N/A'}ï¼Œè¾“å‡ºç»´åº¦ï¼š{layer.out_features if hasattr(layer, 'out_features') else 'N/A'}")
    print(f"ğŸ”§ ä¼˜åŒ–å™¨ï¼šAdamï¼Œå­¦ä¹ ç‡=1e-3")

    # 3. æ³¨å†Œä¸å¯å¯¼å‡½æ•°è§„åˆ™ï¼ˆä»¥torch.roundä¸ºä¾‹ï¼‰
    print(f"\nğŸ“ æ³¨å†Œä¸å¯å¯¼å‡½æ•°è§„åˆ™ï¼š")
    grad_modifier.register_undef_func_rule(
        func_name="round",  # åŒ¹é…RoundBackwardæ¢¯åº¦å‡½æ•°
        grad_func=lambda grad: torch.ones_like(grad) * 0.5  # è‡ªå®šä¹‰æ¢¯åº¦ä¸º0.5
    )

    # 4. ç”Ÿæˆæµ‹è¯•æ•°æ®
    batch_size = 8
    input_data = torch.randn(batch_size, 10)  # è¾“å…¥ï¼š(8,10)
    target = torch.randint(0, 2, (batch_size,))  # æ ‡ç­¾ï¼š0/1åˆ†ç±»
    print(f"\nğŸ“Š æµ‹è¯•æ•°æ®ï¼š")
    print(f"  - è¾“å…¥å½¢çŠ¶ï¼š{input_data.shape}ï¼Œå‡å€¼={input_data.mean().item():.4f}ï¼Œæ ‡å‡†å·®={input_data.std().item():.4f}")
    print(f"  - æ ‡ç­¾å½¢çŠ¶ï¼š{target.shape}ï¼Œæ ‡ç­¾åˆ†å¸ƒï¼š{torch.bincount(target).tolist()}ï¼ˆ0çš„æ•°é‡ï¼Œ1çš„æ•°é‡ï¼‰")

    # 5. å‰å‘ä¼ æ’­ï¼ˆåŒ…å«ä¸å¯å¯¼æ“ä½œtorch.roundï¼‰
    print(f"\nğŸ”„ å‰å‘ä¼ æ’­ï¼š")
    hidden = model[0](input_data)  # ç¬¬ä¸€å±‚è¾“å‡ºï¼š(8,20)
    relu_out = model[1](hidden)  # ReLUè¾“å‡ºï¼š(8,20)
    logits = model[2](relu_out)  # æœ€ç»ˆè¾“å‡ºï¼ˆæœªæ¿€æ´»ï¼‰ï¼š(8,2)
    rounded_logits = torch.round(logits)  # ä¸å¯å¯¼æ“ä½œï¼šå¯¹è¾“å‡ºå–æ•´
    loss = torch.nn.functional.cross_entropy(rounded_logits, target)  # è®¡ç®—æŸå¤±

    print(f"  - ç¬¬ä¸€å±‚è¾“å‡ºï¼ˆLinearï¼‰ï¼šå½¢çŠ¶={hidden.shape}ï¼Œå‡å€¼={hidden.mean().item():.4f}")
    print(f"  - ReLUè¾“å‡ºï¼šå½¢çŠ¶={relu_out.shape}ï¼Œéé›¶å…ƒç´ å æ¯”={((relu_out > 0).sum() / relu_out.numel() * 100):.2f}%")
    print(
        f"  - æœ€ç»ˆlogitsï¼šå½¢çŠ¶={logits.shape}ï¼Œå‡å€¼={logits.mean().item():.4f}ï¼ŒèŒƒå›´=[{logits.min().item():.4f}, {logits.max().item():.4f}]")
    print(f"  - å–æ•´ålogitsï¼šå½¢çŠ¶={rounded_logits.shape}ï¼Œå”¯ä¸€å€¼={torch.unique(rounded_logits).tolist()}")
    print(f"  - æŸå¤±å€¼ï¼š{loss.item():.6f}")

    # 6. æ³¨å†Œä¸å¯å¯¼æ“ä½œçš„è¾“å…¥å¼ é‡ï¼ˆrounded_logitsçš„è¾“å…¥æ˜¯logitsï¼‰
    print(f"\nğŸ“Œ æ³¨å†Œç‰¹æ®Šå¼ é‡ï¼š")
    grad_modifier.register_special_tensor(logits, tensor_name="logitsï¼ˆä¸å¯å¯¼æ“ä½œè¾“å…¥ï¼‰")

    # 7. åå‘ä¼ æ’­ï¼ˆè®¡ç®—æ¢¯åº¦ï¼‰
    print(f"\nğŸ”™ åå‘ä¼ æ’­ï¼š")
    optimizer.zero_grad()  # æ¸…ç©ºå†å²æ¢¯åº¦
    print(f"  - æ¸…ç©ºå†å²æ¢¯åº¦å®Œæˆ")
    loss.backward()  # è®¡ç®—æ¢¯åº¦
    print(f"  - æ¢¯åº¦è®¡ç®—å®Œæˆï¼Œå¼€å§‹æ‰“å°å…³é”®æ¢¯åº¦ä¿¡æ¯ï¼š")

    # æ‰“å°åå‘ä¼ æ’­åçš„åŸå§‹æ¢¯åº¦ï¼ˆæœªä¿®æ”¹å‰ï¼‰
    for name, param in model.named_parameters():
        if param.grad is not None:
            print(f"    - {name}ï¼šæ¢¯åº¦å‡å€¼={param.grad.mean().item():.6f}ï¼ŒèŒƒæ•°={param.grad.norm().item():.4f}")
        else:
            print(f"    - {name}ï¼šæ¢¯åº¦ä¸ºNone")
    # æ‰“å°ä¸å¯å¯¼æ“ä½œè¾“å…¥å¼ é‡çš„åŸå§‹æ¢¯åº¦
    if logits.grad is not None:
        print(f"    - logitsï¼ˆä¸å¯å¯¼è¾“å…¥ï¼‰ï¼šæ¢¯åº¦å‡å€¼={logits.grad.mean().item():.6f}ï¼ŒèŒƒæ•°={logits.grad.norm().item():.4f}")
    else:
        print(f"    - logitsï¼ˆä¸å¯å¯¼è¾“å…¥ï¼‰ï¼šæ¢¯åº¦ä¸ºNone")

    # 8. åº”ç”¨æ¢¯åº¦ä¿®æ”¹ç­–ç•¥ï¼ˆå¤šç­–ç•¥ç»„åˆï¼‰
    print(f"\nğŸ¯ åº”ç”¨æ¢¯åº¦ä¿®æ”¹ç­–ç•¥ï¼š")
    grad_modifier.apply(
        parameters=model.parameters(),
        strategies={
            "clip_by_norm": {"max_norm": 1.5, "norm_type": 2.0},  # èŒƒæ•°è£å‰ª
            "clip_by_value": {"min_val": -0.8, "max_val": 0.8},  # å€¼è£å‰ª
            "filter_small_grads": {"threshold": 1e-3},  # å°æ¢¯åº¦è¿‡æ»¤
            "scale_grads": {"scale_factor": 1.2},  # æ¢¯åº¦ç¼©æ”¾
            "apply_sparsity": {"sparsity_ratio": 0.15},  # æ¢¯åº¦ç¨€ç–åŒ–
            "smooth_grads": {"alpha": 0.85},  # æ¢¯åº¦å¹³æ»‘
            "replace_undef_func_grads": True  # ä¸å¯å¯¼æ¢¯åº¦æ›¿æ¢
        }
    )

    # 9. æ‰“å°ä¿®æ”¹åçš„æœ€ç»ˆæ¢¯åº¦ä¿¡æ¯
    print(f"\nğŸ“Š ä¿®æ”¹åçš„æœ€ç»ˆæ¢¯åº¦ä¿¡æ¯ï¼š")
    print("-" * 50)
    total_params = 0
    total_zero_grads = 0
    for name, param in model.named_parameters():
        if param.grad is not None:
            param_count = param.numel()
            zero_count = (param.grad.data == 0).sum().item()
            zero_ratio = (zero_count / param_count) * 100
            total_params += param_count
            total_zero_grads += zero_count

            print(f"å‚æ•°ï¼š{name}")
            print(f"  - å½¢çŠ¶ï¼š{param.shape}ï¼Œæ€»å…ƒç´ æ•°ï¼š{param_count}")
            print(f"  - æ¢¯åº¦å‡å€¼ï¼š{param.grad.mean().item():.6f}ï¼Œæ¢¯åº¦èŒƒæ•°ï¼š{param.grad.norm().item():.4f}")
            print(f"  - é›¶æ¢¯åº¦æ•°é‡ï¼š{zero_count}ï¼Œé›¶æ¢¯åº¦å æ¯”ï¼š{zero_ratio:.2f}%")
            print(f"  - æ¢¯åº¦èŒƒå›´ï¼š[{param.grad.min().item():.6f}, {param.grad.max().item():.6f}]")
            print()
        else:
            print(f"å‚æ•°ï¼š{name} â†’ æ¢¯åº¦ä¸ºNone\n")

    overall_zero_ratio = (total_zero_grads / total_params) * 100
    print(f"ğŸ“ˆ å…¨å±€æ¢¯åº¦ç»Ÿè®¡ï¼š")
    print(f"  - æ€»å‚æ•°æ•°é‡ï¼š{total_params}")
    print(f"  - æ€»é›¶æ¢¯åº¦æ•°é‡ï¼š{total_zero_grads}")
    print(f"  - å…¨å±€é›¶æ¢¯åº¦å æ¯”ï¼š{overall_zero_ratio:.2f}%")

    # 10. å‚æ•°æ›´æ–°ä¸è®­ç»ƒåæ¨¡å‹çŠ¶æ€
    print(f"\nâš¡ æ‰§è¡Œå‚æ•°æ›´æ–°ï¼š")
    optimizer.step()
    print(f"  - å‚æ•°æ›´æ–°å®Œæˆ")

    # æ‰“å°æ›´æ–°åçš„å‚æ•°å˜åŒ–ï¼ˆå¯¹æ¯”æ›´æ–°å‰åï¼‰
    print(f"\nğŸ” å‚æ•°æ›´æ–°å‰åå˜åŒ–ï¼ˆä»¥ç¬¬ä¸€å±‚æƒé‡ä¸ºä¾‹ï¼‰ï¼š")
    first_layer = model[0]  # å–ç¬¬ä¸€å±‚Linear
    # é‡æ–°è®¡ç®—å‰å‘ä¼ æ’­ï¼ŒæŸ¥çœ‹æ›´æ–°åçš„è¾“å‡ºå˜åŒ–
    updated_hidden = first_layer(input_data)
    hidden_diff = torch.abs(updated_hidden - hidden).mean().item()  # è¾“å‡ºå·®å¼‚å‡å€¼

    print(f"  - ç¬¬ä¸€å±‚æƒé‡æ›´æ–°å‰å‡å€¼ï¼š{hidden.mean().item():.6f}")
    print(f"  - ç¬¬ä¸€å±‚æƒé‡æ›´æ–°åå‡å€¼ï¼š{updated_hidden.mean().item():.6f}")
    print(f"  - ç¬¬ä¸€å±‚è¾“å‡ºå·®å¼‚å‡å€¼ï¼š{hidden_diff:.6f}")

    # è®¡ç®—æ›´æ–°åçš„æŸå¤±ï¼ˆéªŒè¯æ¢¯åº¦ä¿®æ”¹æ•ˆæœï¼‰
    updated_relu = model[1](updated_hidden)
    updated_logits = model[2](updated_relu)
    updated_rounded = torch.round(updated_logits)
    updated_loss = torch.nn.functional.cross_entropy(updated_rounded, target)

    print(f"\nğŸ“‰ æŸå¤±å˜åŒ–ï¼š")
    print(f"  - æ›´æ–°å‰æŸå¤±ï¼š{loss.item():.6f}")
    print(f"  - æ›´æ–°åæŸå¤±ï¼š{updated_loss.item():.6f}")
    print(f"  - æŸå¤±å˜åŒ–é‡ï¼š{updated_loss.item() - loss.item():.6f}")

    # 11. ä¸å¯å¯¼æ¢¯åº¦æ›¿æ¢éªŒè¯
    print(f"\nâœ… ä¸å¯å¯¼æ¢¯åº¦æ›¿æ¢éªŒè¯ï¼š")
    if logits.grad is not None:
        # æ£€æŸ¥logitsæ¢¯åº¦æ˜¯å¦è¢«æ›¿æ¢ä¸º0.5ï¼ˆè‡ªå®šä¹‰è§„åˆ™ï¼‰
        grad_unique = torch.unique(logits.grad)
        is_replaced = torch.allclose(logits.grad, torch.ones_like(logits.grad) * 0.5, atol=1e-6)
        print(f"  - logitsæ¢¯åº¦å”¯ä¸€å€¼ï¼š{[round(v.item(), 6) for v in grad_unique]}")
        print(f"  - æ˜¯å¦ç¬¦åˆè‡ªå®šä¹‰æ¢¯åº¦è§„åˆ™ï¼ˆå…¨éƒ¨ä¸º0.5ï¼‰ï¼š{'æ˜¯' if is_replaced else 'å¦'}")
        print(f"  - logitsæ¢¯åº¦å½¢çŠ¶ï¼š{logits.grad.shape}ï¼Œæ¢¯åº¦æ€»å’Œï¼š{logits.grad.sum().item():.4f}")
    else:
        print(f"  - logitsæ¢¯åº¦ä¸ºNoneï¼Œæ›¿æ¢å¤±è´¥")

    print(f"\n" + "=" * 80)
    print("ğŸ“‹ æµ‹è¯•æ¡ˆä¾‹æ‰§è¡Œå®Œæ¯•ï¼Œæ‰€æœ‰è¿‡ç¨‹ç»“æœå·²æ‰“å°")
    print("=" * 80)
